{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 10\n",
    "\n",
    "Name:  Haya AlMajali, Chris Chan\n",
    "UID: U83334432, U31827126\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Singular Value Decomposition\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "SVD finds features that are orthogonal. The Singular Values correspond to the importance of the feature or how much variance in the data it captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 500\n",
    "C = np.array([[0.1, 0.6], [2., .6]])\n",
    "X = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "plt.title(\"Raw Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.mean(X, axis=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "plt.title(\"Mean-centered Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt=np.linalg.svd(X, full_matrices=False)\n",
    "plt.plot(s) # only 2 singular values\n",
    "plt.title(\"Singular Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopy0 = s.copy()\n",
    "scopy1 = s.copy()\n",
    "scopy0[1:] = 0.0\n",
    "scopy1[:1] = 0.0\n",
    "approx0 = u.dot(np.diag(scopy0)).dot(vt)\n",
    "approx1 = u.dot(np.diag(scopy1)).dot(vt)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "sv1 = np.array([[-5],[5]]) @ vt[[0],:]\n",
    "sv2 = np.array([[-2],[2]]) @ vt[[1],:]\n",
    "plt.plot(sv1[:,0], sv1[:,1], 'g-', label=\"1st sing vector\")\n",
    "plt.plot(sv2[:,0], sv2[:,1], 'g--', label=\"2nd sing vector\")\n",
    "plt.scatter(approx0[:, 0] , approx0[:, 1], s=10, alpha=0.8, color=\"red\", label=\"data projected on 1st SV\")\n",
    "plt.scatter(approx1[:, 0] , approx1[:, 1], s=10, alpha=0.8, color=\"yellow\", label=\"data projected on 2nd SV\")\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title(\"Mean-centered Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ouput from svd is the same\n",
    "orthonormal_X = u\n",
    "shifted_X = u.dot(np.diag(s))\n",
    "plt.axis('equal')\n",
    "plt.scatter(shifted_X[:,0], shifted_X[:,1])\n",
    "plt.scatter(orthonormal_X[:,0], orthonormal_X[:,1])\n",
    "plt.xlabel(\"1st singular vector\")\n",
    "plt.ylabel(\"2nd singular vector\")\n",
    "plt.title(\"data in the new feature space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "boat = np.loadtxt('./boat.dat')\n",
    "plt.figure()\n",
    "plt.imshow(boat, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "a) Plot the singular values of the image above (note: a gray scale image is just a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u,s,vt=np.linalg.svd(boat,full_matrices=False)\n",
    "plt.plot(s)\n",
    "plt.title(\"Singular Values of the Image\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Singular Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice you can get the image back by multiplying the matrices back together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_copy = u.dot(np.diag(s)).dot(vt)\n",
    "plt.figure()\n",
    "plt.imshow(boat_copy, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create a new matrix `scopy` which is a copy of `s` with all but the first singular value set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scopy = s.copy()\n",
    "scopy[1:] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create an approximation of the boat image by multiplying `u`, `scopy`, and `v` transpose. Plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boat_app = u.dot(np.diag(scopy)).dot(vt)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(boat_app, cmap = cm.Greys_r)\n",
    "plt.title('Rank 1 Boat')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(boat, cmap = cm.Greys_r)\n",
    "plt.title('Rank 512 Boat')\n",
    "_ = plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat c) with 40 singular values instead of just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scopy_40 = s.copy()\n",
    "scopy_40[40:] = 0.0\n",
    "boat_app_40 = u.dot(np.diag(scopy_40)).dot(vt)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(boat_app_40, cmap = cm.Greys_r)\n",
    "plt.title('Rank 40 Boat')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(boat, cmap = cm.Greys_r)\n",
    "plt.title('Original Boat')\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you should care\n",
    "\n",
    "a) By using an approximation of the data, you can improve the performance of classification tasks since:\n",
    "\n",
    "1. there is less noise interfering with classification\n",
    "2. no relationship between features after SVD\n",
    "3. the algorithm is sped up when reducing the dimension of the dataset\n",
    "\n",
    "Below is some code to perform facial recognition on a dataset. Notice that, applied blindly, it does not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Get face data\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "# plot face data\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])\n",
    "plt.show()\n",
    "\n",
    "# split train test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n",
    "\n",
    "# blindly fit svm\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=5, gamma=0.001)\n",
    "\n",
    "# fit model\n",
    "model = svc.fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(ytest, yfit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing SVD before applying the classification tool, we can reduce the dimension of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at singular values\n",
    "_, s, _ = np.linalg.svd(Xtrain, full_matrices=False)\n",
    "plt.plot(range(1,len(s)+1),s)\n",
    "plt.title(\"Singular Values\")\n",
    "plt.show()\n",
    "\n",
    "# extract principal components\n",
    "pca = PCA(n_components=..., whiten=True)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=5, gamma=0.001)\n",
    "svcpca = make_pipeline(pca, svc)\n",
    "model = svcpca.fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(ytest, yfit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to finding k in K-means, we're trying to find the point of diminishing returns when picking the number of singular vectors (also called principal components).\n",
    "\n",
    "b) SVD can be used for anomaly detection.\n",
    "\n",
    "The data below consists of the number of 'Likes' during a six month period, for each of 9000 users across the 210 content categories that Facebook assigns to pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/spatial_data.txt')\n",
    "\n",
    "FBSpatial = data[:,1:]\n",
    "FBSnorm = np.linalg.norm(FBSpatial,axis=1,ord=1)\n",
    "plt.plot(FBSnorm)\n",
    "plt.title('Number of Likes Per User')\n",
    "_ = plt.xlabel('Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How users distribute likes across categories follows a general pattern that most users follow. This behavior can be captured using few singular vectors. And anomalous users can be easily identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u,s,vt = np.linalg.svd(FBSpatial,full_matrices=False)\n",
    "plt.plot(s)\n",
    "_ = plt.title('Singular Values of Spatial Like Matrix')\n",
    "plt.show()\n",
    "\n",
    "RANK = ...\n",
    "scopy = s.copy()\n",
    "scopy[RANK:] = 0.\n",
    "N = u @ np.diag(scopy) @ vt\n",
    "O = FBSpatial - N\n",
    "Onorm = np.linalg.norm(O, axis=1)\n",
    "anomSet = np.argsort(Onorm)[-30:]\n",
    "# plt.plot(Onorm)\n",
    "# plt.plot(anomSet, Onorm[anomSet],'ro')\n",
    "# _ = plt.title('Norm of Residual (rows of O)')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(FBSnorm)\n",
    "plt.plot(anomSet, FBSnorm[anomSet],'ro')\n",
    "_ = plt.title('Top 30 Anomalous Users - Total Number of Likes')\n",
    "plt.show()\n",
    "\n",
    "# anomalous users\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in range(1,10):\n",
    "    ax = plt.subplot(3,3,i)\n",
    "    plt.plot(FBSpatial[anomSet[i-1],:])\n",
    "    plt.xlabel('FB Content Categories')\n",
    "plt.subplots_adjust(wspace=0.25,hspace=0.45)\n",
    "_ = plt.suptitle('Nine Example Anomalous Users',size=20)\n",
    "plt.show()\n",
    "\n",
    "# normal users\n",
    "set = np.argsort(Onorm)[0:7000]\n",
    "# that have high overall volume\n",
    "max = np.argsort(FBSnorm[set])[::-1]\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in range(1,10):\n",
    "    ax = plt.subplot(3,3,i)\n",
    "    plt.plot(FBSpatial[set[max[i-1]],:])\n",
    "    plt.xlabel('FB Content Categories')\n",
    "plt.subplots_adjust(wspace=0.25,hspace=0.45)\n",
    "_ = plt.suptitle('Nine Example Normal Users',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem\n",
    "\n",
    "a) Fetch the \"mnist_784\" data. Pick an image of a digit at random and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import random\n",
    "\n",
    "X, y = fetch_openml(name=\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "random_index = random.randint(0, len(X))\n",
    "\n",
    "# your code here\n",
    "plt.imshow(X[random_index].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Digit: {y[random_index]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "b) Plot its singular value plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the MNIST digit image to 2D (28x28)\n",
    "digit_image = X[random_index].reshape(28, 28)\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "u, s, vt = np.linalg.svd(digit_image, full_matrices=False)\n",
    "\n",
    "# Plot the singular values\n",
    "plt.plot(s)\n",
    "plt.title(\"Singular Values of the MNIST Digit Image\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Singular Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "c) By setting some singular values to 0, plot the approximation of the image next to the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the MNIST digit image to 2D (28x28)\n",
    "digit_image = X[4].reshape(28, 28)\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "u, s, vt = np.linalg.svd(digit_image, full_matrices=False)\n",
    "\n",
    "# Choose the number of singular values to keep (e.g., 50)\n",
    "num_singular_values_to_keep = 500\n",
    "\n",
    "# Truncate the singular values and matrices\n",
    "s_truncated = np.zeros_like(s)\n",
    "s_truncated[:num_singular_values_to_keep] = s[:num_singular_values_to_keep]\n",
    "u_truncated = u[:, :num_singular_values_to_keep]\n",
    "vt_truncated = vt[:num_singular_values_to_keep, :]\n",
    "\n",
    "# Reconstruct the approximated image\n",
    "digit_image_approx = u_truncated @ np.diag(s_truncated) @ vt_truncated\n",
    "\n",
    "# Plot the original and approximated images side by side\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(digit_image, cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Approximated image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(digit_image_approx, cmap='gray')\n",
    "plt.title(f\"Approximation (k={num_singular_values_to_keep})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "d) Consider the entire dataset as a matrix. Perform SVD and explain why / how you chose a particular rank. Note: you may not be able to run this on the entire dataset in a reasonable amount of time so you may take a small random sample for this and the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch the \"mnist_784\" dataset\n",
    "X, y = fetch_openml(name=\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Randomly sample a subset of the dataset\n",
    "sample_size = 100  # Adjust as needed\n",
    "random_indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "X_sampled = X[random_indices]\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD)\n",
    "u, s, vt = np.linalg.svd(X_sampled, full_matrices=False)\n",
    "\n",
    "# Plot the singular values\n",
    "plt.plot(s)\n",
    "plt.title(\"Singular Values of Sampled MNIST Dataset\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Singular Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Choose a particular rank based on the singular value plot or explained variance\n",
    "chosen_rank = 2000  # Adjust as needed\n",
    "\n",
    "# Truncate the singular values and matrices\n",
    "s_truncated = np.zeros_like(s)\n",
    "s_truncated[:chosen_rank] = s[:chosen_rank]\n",
    "u_truncated = u[:, :chosen_rank]\n",
    "vt_truncated = vt[:chosen_rank, :]\n",
    "\n",
    "# Reconstruct the approximated dataset\n",
    "X_approx = u_truncated @ np.diag(s_truncated) @ vt_truncated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "I looked at the graph & chose the point where the rate of decrease slows down significantly (around 2000 in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Using Kmeans on this new dataset, cluster the images from d) using 10 clusters and plot the centroid of each cluster. Note: the centroids should be represented as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize and fit KMeans with 10 clusters\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++')\n",
    "cluster_labels = kmeans.fit_predict(X_approx)\n",
    "\n",
    "# Get the centroids of each cluster\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Plot the centroids as images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, centroid in enumerate(centroids):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(centroid.reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Cluster {i+1} Centroid\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) on the original dataset (if you used a subset of the dataset, keep using that same subset). Comment on any differences (or lack thereof) you observe between the centroids created here vs the ones you created in e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize and fit KMeans with 10 clusters on the original dataset or subset\n",
    "kmeans_original = KMeans(n_clusters=10, init='k-means++')\n",
    "cluster_labels_original = kmeans_original.fit_predict(X_sampled)\n",
    "\n",
    "# Get the centroids of each cluster\n",
    "centroids_original = kmeans_original.cluster_centers_\n",
    "\n",
    "# Plot the centroids as images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, centroid_original in enumerate(centroids_original):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(centroid_original.reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Cluster {i+1} Centroid (Original)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Create a matrix (let's call it `O`) that is the difference between the original dataset and the rank-10 approximation of the dataset. i.e. if the original dataset is `A` and the rank-10 approximation is `B`, then `O = A - B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Singular Value Decomposition on the original dataset\n",
    "u_original, s_original, vt_original = np.linalg.svd(X_sampled, full_matrices=False)\n",
    "\n",
    "# Truncate the matrices to rank-10 approximation\n",
    "s_truncated_original = np.zeros_like(s_original)\n",
    "s_truncated_original[:10] = s_original[:10]\n",
    "u_truncated_original = u_original[:, :10]\n",
    "vt_truncated_original = vt_original[:10, :]\n",
    "\n",
    "# Reconstruct the rank-10 approximation of the original dataset\n",
    "X_sample_approx_original = u_original[:, :10] @ np.diag(s_truncated_original[:10]) @ vt_original[:10, :]\n",
    "\n",
    "# Create the matrix O as the difference between the original dataset and the rank-10 approximation\n",
    "O = X_sampled - X_sample_approx_original\n",
    "\n",
    "X_reconstructed = X_sample_approx_original + O\n",
    "reconstruction_error = np.mean((X_sample_approx_original - X_reconstructed)**2)\n",
    "print(\"Reconstruction Error:\", reconstruction_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) The largest (using euclidean distance from the origin) rows of the matrix `O` could be considered anomalous data points. Briefly explain why. Plot the 10 images (by finding them in the original dataset) responsible for the 10 largest rows of that matrix `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance of each row from the origin\n",
    "row_distances = np.linalg.norm(O, axis=1)\n",
    "\n",
    "# Find the indices of the 10 largest rows in terms of distance\n",
    "largest_row_indices = np.argsort(row_distances)[-10:]\n",
    "\n",
    "# Plot the corresponding images from the original dataset\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, index in enumerate(largest_row_indices):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_sampled[index].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Anomalous Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest rows of the matrix `O` could be regarded as anomalous data points because they represent instances where the approximation derived from Singular Value Decomposition significantly deviates from the original data-set. These deviations may result from errors or unusual patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
