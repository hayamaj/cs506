{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 10\n",
    "\n",
    "Name:  \n",
    "UID: \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Singular Value Decomposition\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "SVD finds features that are orthogonal. The Singular Values correspond to the importance of the feature or how much variance in the data it captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 500\n",
    "C = np.array([[0.1, 0.6], [2., .6]])\n",
    "X = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "plt.title(\"Raw Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.mean(X, axis=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "plt.title(\"Mean-centered Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt=np.linalg.svd(X, full_matrices=False)\n",
    "plt.plot(s) # only 2 singular values\n",
    "plt.title(\"Singular Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopy0 = s.copy()\n",
    "scopy1 = s.copy()\n",
    "scopy0[1:] = 0.0\n",
    "scopy1[:1] = 0.0\n",
    "approx0 = u.dot(np.diag(scopy0)).dot(vt)\n",
    "approx1 = u.dot(np.diag(scopy1)).dot(vt)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\n",
    "sv1 = np.array([[-5],[5]]) @ vt[[0],:]\n",
    "sv2 = np.array([[-2],[2]]) @ vt[[1],:]\n",
    "plt.plot(sv1[:,0], sv1[:,1], 'g-', label=\"1st sing vector\")\n",
    "plt.plot(sv2[:,0], sv2[:,1], 'g--', label=\"2nd sing vector\")\n",
    "plt.scatter(approx0[:, 0] , approx0[:, 1], s=10, alpha=0.8, color=\"red\", label=\"data projected on 1st SV\")\n",
    "plt.scatter(approx1[:, 0] , approx1[:, 1], s=10, alpha=0.8, color=\"yellow\", label=\"data projected on 2nd SV\")\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.title(\"Mean-centered Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ouput from svd is the same\n",
    "orthonormal_X = u\n",
    "shifted_X = u.dot(np.diag(s))\n",
    "plt.axis('equal')\n",
    "plt.scatter(shifted_X[:,0], shifted_X[:,1])\n",
    "plt.scatter(orthonormal_X[:,0], orthonormal_X[:,1])\n",
    "plt.xlabel(\"1st singular vector\")\n",
    "plt.ylabel(\"2nd singular vector\")\n",
    "plt.title(\"data in the new feature space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "boat = np.loadtxt('./boat.dat')\n",
    "plt.figure()\n",
    "plt.imshow(boat, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the singular values of the image above (note: a gray scale image is just a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,vt=np.linalg.svd(boat,full_matrices=False)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice you can get the image back by multiplying the matrices back together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_copy = u.dot(np.diag(s)).dot(vt)\n",
    "plt.figure()\n",
    "plt.imshow(boat_copy, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create a new matrix `scopy` which is a copy of `s` with all but the first singular value set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create an approximation of the boat image by multiplying `u`, `scopy`, and `v` transpose. Plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boat_app = ...\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(boat_app, cmap = cm.Greys_r)\n",
    "plt.title('Rank 1 Boat')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(boat, cmap = cm.Greys_r)\n",
    "plt.title('Rank 512 Boat')\n",
    "_ = plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat c) with 40 singular values instead of just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why you should care\n",
    "\n",
    "a) By using an approximation of the data, you can improve the performance of classification tasks since:\n",
    "\n",
    "1. there is less noise interfering with classification\n",
    "2. no relationship between features after SVD\n",
    "3. the algorithm is sped up when reducing the dimension of the dataset\n",
    "\n",
    "Below is some code to perform facial recognition on a dataset. Notice that, applied blindly, it does not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Get face data\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "# plot face data\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])\n",
    "plt.show()\n",
    "\n",
    "# split train test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n",
    "\n",
    "# blindly fit svm\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=5, gamma=0.001)\n",
    "\n",
    "# fit model\n",
    "model = svc.fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(ytest, yfit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing SVD before applying the classification tool, we can reduce the dimension of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at singular values\n",
    "_, s, _ = np.linalg.svd(Xtrain, full_matrices=False)\n",
    "plt.plot(range(1,len(s)+1),s)\n",
    "plt.title(\"Singular Values\")\n",
    "plt.show()\n",
    "\n",
    "# extract principal components\n",
    "pca = PCA(n_components=..., whiten=True)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=5, gamma=0.001)\n",
    "svcpca = make_pipeline(pca, svc)\n",
    "model = svcpca.fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(ytest, yfit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to finding k in K-means, we're trying to find the point of diminishing returns when picking the number of singular vectors (also called principal components).\n",
    "\n",
    "b) SVD can be used for anomaly detection.\n",
    "\n",
    "The data below consists of the number of 'Likes' during a six month period, for each of 9000 users across the 210 content categories that Facebook assigns to pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/spatial_data.txt')\n",
    "\n",
    "FBSpatial = data[:,1:]\n",
    "FBSnorm = np.linalg.norm(FBSpatial,axis=1,ord=1)\n",
    "plt.plot(FBSnorm)\n",
    "plt.title('Number of Likes Per User')\n",
    "_ = plt.xlabel('Users')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How users distribute likes across categories follows a general pattern that most users follow. This behavior can be captured using few singular vectors. And anomalous users can be easily identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u,s,vt = np.linalg.svd(FBSpatial,full_matrices=False)\n",
    "plt.plot(s)\n",
    "_ = plt.title('Singular Values of Spatial Like Matrix')\n",
    "plt.show()\n",
    "\n",
    "RANK = ...\n",
    "scopy = s.copy()\n",
    "scopy[RANK:] = 0.\n",
    "N = u @ np.diag(scopy) @ vt\n",
    "O = FBSpatial - N\n",
    "Onorm = np.linalg.norm(O, axis=1)\n",
    "anomSet = np.argsort(Onorm)[-30:]\n",
    "# plt.plot(Onorm)\n",
    "# plt.plot(anomSet, Onorm[anomSet],'ro')\n",
    "# _ = plt.title('Norm of Residual (rows of O)')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(FBSnorm)\n",
    "plt.plot(anomSet, FBSnorm[anomSet],'ro')\n",
    "_ = plt.title('Top 30 Anomalous Users - Total Number of Likes')\n",
    "plt.show()\n",
    "\n",
    "# anomalous users\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in range(1,10):\n",
    "    ax = plt.subplot(3,3,i)\n",
    "    plt.plot(FBSpatial[anomSet[i-1],:])\n",
    "    plt.xlabel('FB Content Categories')\n",
    "plt.subplots_adjust(wspace=0.25,hspace=0.45)\n",
    "_ = plt.suptitle('Nine Example Anomalous Users',size=20)\n",
    "plt.show()\n",
    "\n",
    "# normal users\n",
    "set = np.argsort(Onorm)[0:7000]\n",
    "# that have high overall volume\n",
    "max = np.argsort(FBSnorm[set])[::-1]\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in range(1,10):\n",
    "    ax = plt.subplot(3,3,i)\n",
    "    plt.plot(FBSpatial[set[max[i-1]],:])\n",
    "    plt.xlabel('FB Content Categories')\n",
    "plt.subplots_adjust(wspace=0.25,hspace=0.45)\n",
    "_ = plt.suptitle('Nine Example Normal Users',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem\n",
    "\n",
    "a) Fetch the \"mnist_784\" data. Pick an image of a digit at random and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(name=\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot its singular value plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) By setting some singular values to 0, plot the approximation of the image next to the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Consider the entire dataset as a matrix. Perform SVD and explain why / how you chose a particular rank. Note: you may not be able to run this on the entire dataset in a reasonable amount of time so you may take a small random sample for this and the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Using Kmeans on this new dataset, cluster the images from d) using 10 clusters and plot the centroid of each cluster. Note: the centroids should be represented as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=..., init='k-means++')\n",
    "kmeans.fit_predict(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) on the original dataset (if you used a subset of the dataset, keep using that same subset). Comment on any differences (or lack thereof) you observe between the centroids created here vs the ones you created in e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Create a matrix (let's call it `O`) that is the difference between the original dataset and the rank-10 approximation of the dataset. i.e. if the original dataset is `A` and the rank-10 approximation is `B`, then `O = A - B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) The largest (using euclidean distance from the origin) rows of the matrix `O` could be considered anomalous data points. Briefly explain why. Plot the 10 images (by finding them in the original dataset) responsible for the 10 largest rows of that matrix `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
