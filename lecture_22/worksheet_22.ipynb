{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 22\n",
    "\n",
    "Name:  \n",
    "UID: \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Gradient Descent\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Recall in Linear Regression we are trying to find the line $$y = X \\beta$$ that minimizes the sum of square distances between the predicted `y` and the `y` we observed in our dataset:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2$$\n",
    "\n",
    "We were able to find a global minimum to this loss function but we will try to apply gradient descent to find that same solution.\n",
    "\n",
    "a) Implement the `loss` function to complete the code and plot the loss as a function of beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta = np.array([ 1 , .5 ])\n",
    "xlin = -10.0 + 20.0 * np.random.random(100)\n",
    "X = np.column_stack([np.ones((len(xlin), 1)), xlin])\n",
    "y = beta[0]+(beta[1]*xlin)+np.random.randn(100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xlin, y,'ro',markersize=4)\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(min(y), max(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = np.arange(-5, 4, 0.1)\n",
    "b1 = np.arange(-5, 4, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "def loss(X, y, beta):\n",
    "    return ...\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    " \n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(14, 9))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-6, 4)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-5, 4)\n",
    "ax.set_zlim(0, 30000)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the loss is\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2 = \\beta^T X^T X \\beta - 2\\mathbf{\\beta}^TX^T\\mathbf{y}  + \\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = 2X^T X \\beta - 2X^T\\mathbf{y}$$\n",
    "\n",
    "b) Implement the gradient function below and complete the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(20, -20)\n",
    "    ax.set_xlim(-5, 4)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-5, 4)\n",
    "    ax.set_zlim(0, 30000)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    return ...\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        beta_hat = ...\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.0002 # try .0005\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the code above to create an animation of the linear model learned at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_model(beta):\n",
    "    xplot = np.linspace(-10,10,50)\n",
    "    yestplot = ...\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xplot, yestplot,'b-',lw=2)\n",
    "    ax.plot(xlin, y,'ro',markersize=4)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(min(y), max(y))\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(...)\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 100, images)\n",
    "\n",
    "images[0].save(\n",
    "    'model.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the `loss` is the negative log-likelihood\n",
    "\n",
    "$$ \\mathcal{l}(\\mathbf{\\beta}) = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\sigma(x_i \\beta)) + (1 - y_i)\\log(1 - \\sigma(x_i \\beta))$$\n",
    "\n",
    "the gradient of which is:\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{l}(\\mathbf{\\beta}) = \\frac{1}{N} \\sum_{i=1}^{N} x_i (y_i - \\sigma(x_i \\beta)) $$\n",
    "\n",
    "d) Plot the loss as a function of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "centers = [[0, 0]]\n",
    "t, _ = datasets.make_blobs(n_samples=100, centers=centers, cluster_std=2, random_state=0)\n",
    "\n",
    "# LINE\n",
    "def generate_line_data():\n",
    "    # create some space between the classes\n",
    "    X = t\n",
    "    Y = np.array([1 if x[0] - x[1] >= 0 else 0 for x in X])\n",
    "    return X, Y\n",
    "\n",
    "X, y = generate_line_data()\n",
    "\n",
    "cs = np.array([x for x in 'gb'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], color=cs[y].tolist(), s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = np.arange(-20, 20, 0.1)\n",
    "b1 = np.arange(-20, 20, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(x)\n",
    "    return e / (1 + e)\n",
    "\n",
    "\n",
    "def loss(X, y, beta):\n",
    "    ...\n",
    "\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(14, 9))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-20, 20)\n",
    "ax.set_zlim(0, 10)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the loss at each iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(10, 10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-20, 20)\n",
    "    ax.set_zlim(0, 10)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    return ...\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.1\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd_logit.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Create an animation of the logistic regression fit at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Modify the above code to evaluate the gradient on a random batch of the data. Overlay the true loss curve and the approximation of the loss in your animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Below is a sandox where you can get intuition about how to tune gradient descent parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(x, y, pts, losses, grad):\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(20, -20)\n",
    "    ax.plot_surface(x, y, loss(np.array([x, y])), color='r', alpha=.4)\n",
    "    ax.plot(np.array(pts)[:,0], np.array(pts)[:,1], losses, 'o-', c='b', markersize=10, zorder=10)\n",
    "    ax.plot(np.array(pts)[-1,0], np.array(pts)[-1,1], -1, 'o-', c='b', alpha=.5, markersize=7, zorder=10)\n",
    "    \n",
    "    # Plot Gradient Vector\n",
    "    X, Y, Z = [pts[-1][0]], [pts[-1][1]], [-1]\n",
    "    U, V, W = [-grad[0]], [-grad[1]], [0]\n",
    "    ax.quiver(X, Y, Z, U, V, W, color='g')\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def loss(x):\n",
    "    return np.sin(sum(x**2)) # change this\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * x * np.cos(sum(x**2)) # change this\n",
    "\n",
    "def gradient_descent(x, y, init, learning_rate, epochs):\n",
    "    images, losses, pts = [], [loss(init)], [init]\n",
    "    for _ in range(epochs):\n",
    "        grad = gradient(init)\n",
    "        images.append(snap(x, y, pts, losses, grad))\n",
    "        init = init - learning_rate * grad\n",
    "        losses.append(loss(init))\n",
    "        pts.append(init)\n",
    "    return images\n",
    "\n",
    "init = np.array([-.5, -.5]) # change this\n",
    "learning_rate = 1.394 # change this\n",
    "x, y = np.meshgrid(np.arange(-2, 2, 0.1), np.arange(-2, 2, 0.1)) # change this\n",
    "images = gradient_descent(x, y, init, learning_rate, 12)\n",
    "\n",
    "images[0].save(\n",
    "    'gradient_descent.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
