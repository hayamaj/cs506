{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 14\n",
    "\n",
    "Name:  Haya AlMajali, Chris Chan\n",
    "UID: U83334432, U31827126\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No)\n",
    "- P(Attribute B = Divorced | Class = Yes)\n",
    "- P(Attribute C = High | Class = No)\n",
    "- P(Attribute C = Mid | Class = Yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probability(Attribute A = Yes | Class = No): 0.4\n",
    "- Probability(Attribute B = Divorced | Class = Yes): 0.6667\n",
    "- Probability(Attribute C = High | Class = No): 0.4\n",
    "- Probability(Attribute C = Mid | Class = Yes): 0.6667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid)\n",
    "- (No, Divorced, High)\n",
    "- (No, Single, High)\n",
    "- (No, Divorced, Low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Record 1 (Yes, Married, Mid): No. \n",
    "\n",
    "Probability(Class = No | Attribute A = Yes, Attribute B = Married, Attribute C = Mid): 0.0144\n",
    "Probability(Class = Yes | Attribute A = Yes, Attribute B = Married, Attribute C = Mid): 0\n",
    "\n",
    "Probability for (Class = No) is higher, so we classify it as a No.\n",
    "\n",
    "- Record 2 (No, Divorced, High): Yes\n",
    "\n",
    "Probability(Class = No | Attribute A = No, Attribute B = Divorced, Attribute C = High): 0.0288\n",
    "Probability(Class = Yes | Attribute A = No, Attribute B = Divorced, Attribute C = High): 0.0427\n",
    "\n",
    "Probability for (Class = Yes) is higher, so we classify it as a Yes.\n",
    "    \n",
    "- Record 3 (No, Single, High): No\n",
    "\n",
    "Probability(Class = No | Attribute A = No, Attribute B = Single, Attribute C = High): 0.0288\n",
    "Probability(Class = Yes | Attribute A = No, Attribute B = Single, Attribute C = High): 0.0213\n",
    "\n",
    "Probability for (Class = No) is higher, so we classify it as a No.\n",
    "    \n",
    "- Record 4 (No, Divorced, Low): No\n",
    "\n",
    "Probability(Class = No | Attribute A = No, Attribute B = Divorced, Attribute C = Low): 0.0475\n",
    "Probability(Class = Yes | Attribute A = No, Attribute B = Divorced, Attribute C = Low): 0.0213\n",
    "\n",
    "Probability for (Class = No) is higher, so we classify it as a No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    # initialize variables for true positives, true negatives, false positives, and false negatives\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    # calculate the confusion matrix metrics\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        if act == \"Yes\" and pred == \"Yes\":\n",
    "            tp += 1\n",
    "        elif act == \"No\" and pred == \"No\":\n",
    "            tn += 1\n",
    "        elif act == \"No\" and pred == \"Yes\":\n",
    "            fp += 1\n",
    "        elif act == \"Yes\" and pred == \"No\":\n",
    "            fn += 1\n",
    "    # construct the confusion matrix as a list of lists\n",
    "    confusion_matrix = [[tn, fp], [fn, tp]]\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "print(confusion_matrix(actual_class, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the confusion matrix: [[4, 3], [1, 2]], and the cost matrix above:\n",
    "\n",
    "Cost = (tps * Cost of Predicting Yes as Yes) + (fps * Cost of Predicting No as Yes) + (fns * Cost of Predicting Yes as No) + (tns * Cost of Predicting No as No) = (2 * -1) + (3 * 10) + (1 * 5) + (4 * 0) = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cost(actual, predicted, cost_matrix):\n",
    "    # initializing the cost to zero\n",
    "    cost = 0\n",
    "    \n",
    "    # iterating through the actual and predicted values to calculate the cost\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        if act == \"Yes\" and pred == \"Yes\":\n",
    "            cost += cost_matrix[0][0]  # yes as a yes\n",
    "        elif act == \"No\" and pred == \"Yes\":\n",
    "            cost += cost_matrix[1][0]  # no as a yes\n",
    "        elif act == \"Yes\" and pred == \"No\":\n",
    "            cost += cost_matrix[0][1]  # yes as a no\n",
    "        elif act == \"No\" and pred == \"No\":\n",
    "            cost += cost_matrix[1][1]  # no as a no\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "# the given cost matrix\n",
    "cost_matrix = [[-1, 5],\n",
    "               [10, 0]]\n",
    "\n",
    "total_cost = calculate_cost(actual_class, predicted_class, cost_matrix)\n",
    "\n",
    "print(\"Total Cost:\", total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Implement functions for the following:\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f-measure\n",
    "\n",
    "and apply them to the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    total = len(actual)\n",
    "    for act, pred in zip(actual, predicted):\n",
    "        if act == pred:\n",
    "            correct += 1\n",
    "    return (correct / total)\n",
    "\n",
    "def precision(actual, predicted, target_class):\n",
    "    true_positive = sum(act == target_class and pred == target_class for act, pred in zip(actual, predicted))\n",
    "    false_positive = sum(act != target_class and pred == target_class for act, pred in zip(actual, predicted))\n",
    "    return true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "\n",
    "def recall(actual, predicted, target_class):\n",
    "    true_positive = sum(act == target_class and pred == target_class for act, pred in zip(actual, predicted))\n",
    "    false_negative = sum(act == target_class and pred != target_class for act, pred in zip(actual, predicted))\n",
    "    return true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "\n",
    "def f_measure(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "#given the following\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "#calculate results\n",
    "acc = accuracy(actual_class, predicted_class)\n",
    "prec = precision(actual_class, predicted_class, \"Yes\")\n",
    "rec = recall(actual_class, predicted_class, \"Yes\")\n",
    "f_measure_val = f_measure(prec, rec)\n",
    "\n",
    "#print results\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F-measure:\", f_measure_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful code for the midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Get face data\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "# plot face data\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])\n",
    "plt.show()\n",
    "\n",
    "# split train test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n",
    "\n",
    "pca = PCA(n_components=150, whiten=True)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "svcpca = make_pipeline(pca, svc)\n",
    "\n",
    "# Tune model to find best values of C and gamma using cross validation\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "kfold = 10\n",
    "grid = GridSearchCV(svcpca, param_grid, cv=kfold)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "# use the best params explicitly here\n",
    "pca = PCA(n_components=150, whiten=True)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=0.005)\n",
    "svcpca = make_pipeline(pca, svc)\n",
    "\n",
    "model = BaggingClassifier(svcpca, n_estimators=100).fit(Xtrain, ytrain)\n",
    "yfit = model.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots(6, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14)\n",
    "plt.show()\n",
    "\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(ytest, yfit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
