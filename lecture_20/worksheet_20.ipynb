{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 20\n",
    "\n",
    "Name:  \n",
    "UID: \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Confidence Intervals\n",
    "- Checking Assumptions\n",
    "\n",
    "### Confidence Intervals\n",
    "\n",
    "From worksheet 18, we know that, provided the assumptions from linear regression hold:\n",
    "\n",
    "$$\\hat\\beta \\sim \\mathcal{N}(\\beta,\\sigma^2 (X^TX)^{-1})$$\n",
    "\n",
    "thus for each component $k$ of $\\hat\\beta$\n",
    "\n",
    "$$\\hat\\beta_k \\sim \\mathcal{N}(\\beta_k, \\sigma^2 S_{kk})$$\n",
    "\n",
    "where $S_{kk}$ is the $k^\\text{th}$ diagonal element of $(X^TX)^{-1}$. \n",
    "\n",
    "While we know that the estimate of beta is an unbiased estimator, we don't know the standard deviation. So in practice we use an unbiased estimate of the standard deviation instead. This estimate is the standard error `s`\n",
    "\n",
    "$$s = \\sqrt{\\frac{RSS}{n - p}}$$\n",
    "\n",
    "where p is the number of parameters beta (the intercept counts as one such parameter). This normalized $\\hat\\beta$ can be shown to follow a t-distribution with `n-p` degrees of freedom. Below we can see this is the case through a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLE_SIZE = 10\n",
    "\n",
    "x = -1.0 + 1.0 * np.random.random(SAMPLE_SIZE)\n",
    "y = 0.0 * x + np.random.randn(SAMPLE_SIZE)\n",
    "intercept = np.ones(np.shape(x)[0])\n",
    "X = np.array([intercept, x]).T\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "xs = np.linspace(-1, 0, 100)\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xs, beta_hat[0] + beta_hat[1] * xs, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standard_error(ytrue, ypred):\n",
    "    return (sum((ytrue - ypred)**2) / (len(ytrue) - 2)) ** (1/2)\n",
    "\n",
    "def normalized(beta, mu, sig, skk):\n",
    "    return (beta - mu) / (((sig ** 2) * skk) ** (1/2))\n",
    "\n",
    "SAMPLE_SIZE = 100\n",
    "TRIALS = 10000\n",
    "\n",
    "beta_hist = []\n",
    "for _ in range(TRIALS):\n",
    "    x = -1.0 + 1.0 * np.random.random(SAMPLE_SIZE)\n",
    "    y = 0.0 * x + np.random.randn(SAMPLE_SIZE)\n",
    "    intercept = np.ones(np.shape(x)[0])\n",
    "    X = np.array([intercept, x]).T\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    skk = np.linalg.inv(X.T @ X)[1][1]\n",
    "    s = standard_error(y, X @ beta)\n",
    "    beta_hist.append(normalized(beta[1], 0, s, skk))\n",
    "\n",
    "xs = np.linspace(-5,5,1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(beta_hist, bins=100, density=True)\n",
    "ax.plot(xs, t.pdf(xs, SAMPLE_SIZE - 2), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that normalized $\\hat\\beta$ is less than 0 can be found by evaluating the CDF of the t-distribution above. This clearly evaluates to .5.\n",
    "\n",
    "a) What is the probability that normalized $\\hat\\beta$ is less than -2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cdf(0, SAMPLE_SIZE -2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Still assuming that there is no relationship between X and Y. What value would the normalized $\\hat\\beta$ only be less than ~1% of the time? Another way to phrase this is: what value would the normalized $\\hat\\beta$ be greater than 99% of the time? i.e.\n",
    "\n",
    "$$\\int_{-\\infty}^{x} t_{pdf}(t)dt = .01 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def search(alpha, df):\n",
    "    d = 4\n",
    "    epsilon = .001\n",
    "    delta = 1 * 10 ** (-d)\n",
    "    i = -5\n",
    "    while i < 5:\n",
    "        i += epsilon\n",
    "        val = t.cdf(i, df)\n",
    "        if abs(alpha - val) < delta:\n",
    "            return round(i, ndigits=d-2)\n",
    "    ValueError(\"couldn't find anything\")\n",
    "\n",
    "t_alpha = search(.01, SAMPLE_SIZE - 2)\n",
    "print(t_alpha)\n",
    "print(t.cdf(t_alpha, SAMPLE_SIZE - 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What value of $\\hat\\beta$ does the above translate to? i.e. what value would $\\hat\\beta$ only be less than 1% of the time (assuming there is no relationship between X and Y)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skk = np.linalg.inv(X.T @ X)[1][1]\n",
    "s = standard_error(y, X @ beta)\n",
    "t_alpha = search(.01, SAMPLE_SIZE - 2)\n",
    "beta_hat = t_alpha * ((s ** 2) * skk) ** (1/2)\n",
    "\n",
    "print(t_alpha)\n",
    "print(beta_hat)\n",
    "print(t.cdf(normalized(beta_hat, 0, s, skk), SAMPLE_SIZE - 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) So what range of values [-t, t] is expected to hold $\\hat\\beta$ ~98% of the time?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-.788, .788]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Verify this by running a simulation where you fix the interval and record the number of times in a given number of TRIALS that $\\hat\\beta$ falls in the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_min = -.788\n",
    "interval_max = .788\n",
    "\n",
    "TRIALS = 10000\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "count = 0\n",
    "for _ in range(TRIALS):\n",
    "    x = -1.0 + 1.0 * np.random.random(SAMPLE_SIZE)\n",
    "    y = 0.0 * x + np.random.randn(SAMPLE_SIZE)\n",
    "    intercept = np.ones(np.shape(x)[0])\n",
    "    X = np.array([intercept, x]).T\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    if beta[1] > interval_min and beta[1] < interval_max :\n",
    "        count += 1\n",
    "\n",
    "print(count / TRIALS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) For the dataset below, report the 99% confidence interval around the estimate of the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-0.1920605, -0.11290798, -0.56434374, -0.67052057, -0.19233284, -0.42403586, -0.8114285, -0.38986946, -0.37384161, -0.50930229])\n",
    "y = np.array([-0.34063108, -0.33409286, 0.34245857, 0.11062295, 0.76682389, 0.86592388, -1.68912015, -2.01463592, 1.61798563, 0.60557414])\n",
    "\n",
    "intercept = np.ones(np.shape(x)[0])\n",
    "X = np.array([intercept, x]).T\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(beta_hat)\n",
    "\n",
    "skk = np.linalg.inv(X.T @ X)[1][1]\n",
    "s = standard_error(y, X @ beta)\n",
    "t_alpha = search(.2, len(x) - 2)\n",
    "\n",
    "print(\"60% CI around the slope is: [\", beta_hat[1] + t_alpha * ((s ** 2) * skk) ** (1/2) , \", \", beta_hat[1] - t_alpha * ((s ** 2) * skk) ** (1/2) ,\"]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Based on this confidence interval, what can you deduce about the relationship between x and y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Assumptions\n",
    "\n",
    "The 50% quantile of $\\mathcal{N}(0, \\sigma^2)$ is 0 because 50% of the values are below 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "xs = np.linspace(-5,5,1000)\n",
    "xs_upto = np.linspace(-5,0,1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs, norm.pdf(xs, 0, 1), color='blue', label='N(0, 1)')\n",
    "ax.fill_between(xs_upto, norm.pdf(xs_upto, 0, 1), color='blue', alpha=0.3)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the 25% quantile of the $\\mathcal{N}(0, 1)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_quantile(q, mu, sigma):\n",
    "    d = 3\n",
    "    epsilon = .001\n",
    "    delta = 1 * 10 ** (-d)\n",
    "    i = -5\n",
    "    while i < 5:\n",
    "        i += epsilon\n",
    "        val = norm.cdf(i, mu, sigma)\n",
    "        if abs(q - val) < delta:\n",
    "            return round(i, ndigits=d-1)\n",
    "    ValueError(\"couldn't find anything\")\n",
    "\n",
    "print(theoretical_quantile(.25, 0, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What is the 25% quantile of the following sample from a $\\mathcal{N}(0, 1)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 100\n",
    "\n",
    "sample = norm.rvs(0, 1, size=SAMPLE_SIZE)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sample, bins=25)\n",
    "plt.show()\n",
    "\n",
    "def sample_quantile(q, sample):\n",
    "    sorted_sample = np.sort(sample)\n",
    "    count = 0\n",
    "    for i in range(len(sorted_sample)):\n",
    "        count += 1\n",
    "        if count / len(sample) >= q :\n",
    "            return sorted_sample[i]\n",
    "    raise ValueError(\"unable to find quantile\")\n",
    "\n",
    "print(sample_quantile(.5, sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create a scatter plot where the x axis has the theoretical quantiles from the normal distribution and the y axis has the sample quantiles from the data above. You can plot every tenth quantile to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUANT = 20\n",
    "quants = [x / NUM_QUANT for x in range(1, NUM_QUANT)]\n",
    "\n",
    "theory_quants = [theoretical_quantile(x, 0, 1) for x in quants]\n",
    "sample_quants = [sample_quantile(x, sample) for x in quants]\n",
    "\n",
    "xs = np.linspace(-3,3,1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(theory_quants, sample_quants)\n",
    "ax.plot(xs, xs)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, in linear regression we assume that $Y = X\\beta + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. $\\epsilon = Y - X\\beta$ but we can't just plot $\\epsilon$ values to see if they follow a normal distribution because we don't know the true $\\beta$ with which to multiply the X's in our dataset... Luckily, since $\\hat\\beta$ is an unbiased estimator of $\\beta$ we can use $e = Y - X\\hat\\beta$ instead. These are called residuals and are estimates of $\\epsilon$.\n",
    "\n",
    "d) Describe how you would check that the normality assumption holds in the dataset you were given?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, maybe more subtle assumption, is that the variance of the errors is constant (i.e. it does not depend on the value of X). Below are examples of datasets generated with and without constant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0 + 10.0 * np.random.random(SAMPLE_SIZE)\n",
    "y = 1 + 3 * x + np.random.randn(SAMPLE_SIZE)\n",
    "intercept = np.ones(np.shape(x)[0])\n",
    "X = np.array([intercept, x]).T\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "s = standard_error(y, X @ beta)\n",
    "\n",
    "xs = np.linspace(0,10,1000)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.scatter(x, y)\n",
    "ax1.plot(xs, beta[0] + beta[1] * xs, color='red')\n",
    "ax1.plot(xs, 3 * s ** 2 + beta[0] + beta[1] * xs, c='r', linestyle='dashed', label='$3\\sigma^2$')\n",
    "ax1.plot(xs, - 3 * s ** 2 + beta[0] + beta[1] * xs, c='r', linestyle='dashed')\n",
    "ax1.set_title(\"Constant Variance\")\n",
    "ax1.legend()\n",
    "\n",
    "y1 = 1 + 3 * x + x * np.random.randn(SAMPLE_SIZE)\n",
    "intercept = np.ones(np.shape(x)[0])\n",
    "X = np.array([intercept, x]).T\n",
    "beta1 = np.linalg.inv(X.T @ X) @ X.T @ y1\n",
    "s = standard_error(y1, X @ beta1)\n",
    "\n",
    "space = 2\n",
    "ax2.scatter(x, y1)\n",
    "ax2.plot(xs, beta1[0] + beta1[1] * xs, color='red')\n",
    "ax2.plot(xs, space + xs + beta1[0] + beta1[1] * xs, c='r', linestyle='dashed')\n",
    "ax2.plot(xs, - space - xs + beta1[0] + beta1[1] * xs, c='r', linestyle='dashed')\n",
    "ax2.set_title(\"Non-Constant Variance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can easily inspect these plots to check for constant variance, this is only because we have 2-dimensional data that we can actually visualize. In practice, there may be any number of features / explanatory variables so we need a different way to check this assumption. Recall the residuals should follow a normal distribution with constant variance. These residuals are always 1-dimensional since $e = Y - X \\hat \\beta$. But we can't plot wrt X since X is multi-dimensional. So the next best thing is to plot wrt $X \\hat \\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 35,1000)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.scatter(X @ beta, y - X @ beta)\n",
    "ax1.plot(xs, np.zeros_like(xs), color='red')\n",
    "ax1.set_title(\"Residuals vs Fitted Values (Constant Variance)\")\n",
    "\n",
    "ax2.scatter(X @ beta1, y1 - X @ beta1)\n",
    "ax2.plot(xs, np.zeros_like(xs), c='r')\n",
    "ax2.set_title(\"Residuals vs Fitted Values (Non-Constant Variance)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-constant variance is bad because it means the uncertainty of our prediction depends on what we are trying to predict. For example above if x is close to 0 then we will be closer to the true y than if x is closer to say 10.\n",
    "\n",
    "If we run into non-constant variance in practice, we may be able to transform Y to lessen the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 4, 1000)\n",
    "fig, ax = plt.subplots()\n",
    "new_beta = np.linalg.inv(X.T @ X) @ X.T @ np.log(y1)\n",
    "ax.scatter(X @ new_beta, np.log(y1) - X @ new_beta)\n",
    "ax.plot(xs, np.zeros_like(xs), c='r')\n",
    "ax.set_title(\"Using log(Y)\")\n",
    "plt.show()\n",
    "\n",
    "xs = np.linspace(1.5, 6, 1000)\n",
    "fig, ax = plt.subplots()\n",
    "new_beta = np.linalg.inv(X.T @ X) @ X.T @ np.sqrt(y1)\n",
    "ax.scatter(X @ new_beta, np.sqrt(y1) - X @ new_beta)\n",
    "ax.plot(xs, np.zeros_like(xs), c='r')\n",
    "ax.set_title(\"Using sqrt(Y)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models\n",
    "\n",
    "What happens when the errors are not normally distributed? This is the topic of GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLE_SIZE = 100\n",
    "x = 0 + 10.0 * np.random.random(SAMPLE_SIZE)\n",
    "y = 1 + 3 * x + np.random.gamma(10, 5, SAMPLE_SIZE)\n",
    "intercept = np.ones(np.shape(x)[0])\n",
    "X = np.array([intercept, x]).T\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "xs = np.linspace(0,10,1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(xs, beta[0] + beta[1] * xs, color='red') # learned line\n",
    "ax.plot(xs, 1 + 3 * xs, c='b') # true line\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
