{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 21\n",
    "\n",
    "Name:  Haya AlMajali\n",
    "UID: U83334432\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Logistic Regression\n",
    "- Gradient Descent\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "centers = [[0, 0]]\n",
    "t, _ = datasets.make_blobs(n_samples=750, centers=centers, cluster_std=1, random_state=0)\n",
    "\n",
    "# LINE\n",
    "def generate_line_data():\n",
    "    # create some space between the classes\n",
    "    X = np.array(list(filter(lambda x : x[0] - x[1] < -.5 or x[0] - x[1] > .5, t)))\n",
    "    Y = np.array([1 if x[0] - x[1] >= 0 else 0 for x in X])\n",
    "    return X, Y\n",
    "\n",
    "# CIRCLE\n",
    "def generate_circle_data(t):\n",
    "    # create some space between the classes\n",
    "    X = np.array(list(filter(lambda x : (x[0] - centers[0][0])**2 + (x[1] - centers[0][1])**2 < 1 or (x[0] - centers[0][0])**2 + (x[1] - centers[0][1])**2 > 1.5, t)))\n",
    "    Y = np.array([1 if (x[0] - centers[0][0])**2 + (x[1] - centers[0][1])**2 >= 1 else 0 for x in X])\n",
    "    return X, Y\n",
    "\n",
    "# XOR\n",
    "def generate_xor_data():\n",
    "    X = np.array([\n",
    "        [0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]])\n",
    "    Y = np.array([x[0]^x[1] for x in X])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "a) Using the above code, generate and plot data that is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_line, Y_line = generate_line_data()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_line[:, 0], X_line[:, 1], c=Y_line, cmap=plt.cm.coolwarm, s=20)\n",
    "plt.title('Linearly Separable Data')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "b) Fit a logistic regression model to the data a print out the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_line, Y_line)\n",
    "model.coef_\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "c) Using the coefficients, plot the line through the scatter plot you created in a). (Note: you need to do some math to get the line in the right form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit logistic regression model\n",
    "X_line, Y_line = generate_line_data()\n",
    "model = LogisticRegression().fit(X_line, Y_line)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "coef = model.coef_[0]\n",
    "intercept = model.intercept_[0]\n",
    "\n",
    "# Calculate slope and y-intercept of the decision boundary line\n",
    "slope = -coef[0] / coef[1]\n",
    "y_intercept = -intercept / coef[1]\n",
    "\n",
    "# Plot the data and decision boundary line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_line[:, 0], X_line[:, 1], c=Y_line, cmap=plt.cm.coolwarm, s=20)\n",
    "plt.plot([X_line[:, 0].min(), X_line[:, 0].max()], [slope * X_line[:, 0].min() + y_intercept, slope * X_line[:, 0].max() + y_intercept], color='k', linestyle='-', linewidth=2)\n",
    "plt.title('Linearly Separable Data with Decision Boundary')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "d) Using the above code, generate and plot the CIRCLE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate circular data\n",
    "X_circle, Y_circle = generate_circle_data(t)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_circle[:, 0], X_circle[:, 1], c=Y_circle, cmap=plt.cm.coolwarm, s=20)\n",
    "plt.title('Circular Data')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "e) Notice that the equation of an ellipse is of the form $$ax^2 + by^2 = c$$\n",
    "\n",
    "Fit a logistic regression model to an appropriate transformation of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate circular data\n",
    "X_circle, Y_circle = generate_circle_data(t)\n",
    "\n",
    "# Perform feature transformation to create elliptical features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_elliptical = poly.fit_transform(X_circle)\n",
    "\n",
    "# Fit logistic regression model to the transformed data\n",
    "model = LogisticRegression().fit(X_elliptical, Y_circle)\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "f) Plot the decision boundary using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a mesh to plot in\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X_circle[:, 0].min() - .5, X_circle[:, 0].max() + .5\n",
    "y_min, y_max = X_circle[:, 1].min() - .5, X_circle[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "A = model.predict_proba(poly.transform(meshData))[:, 1].reshape(xx.shape)\n",
    "Z = model.predict(poly.transform(meshData)).reshape(xx.shape)\n",
    "ax.contourf(xx, yy, A, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_circle[:, 0], X_circle[:, 1], c=Y_circle, s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "g) Plot the XOR data. In this 2D space, the data is not linearly separable, but by introducing a new feature $$x_3 = x_1 * x_2$$\n",
    "\n",
    "(called an interaction term) we should be able to find a hyperplane that separates the data in 3D. Plot this new dataset in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X, Y = generate_xor_data()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X[: , 0], X[: , 1], X[: , 0]* X[: , 1], c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "h) Apply a logistic regression model using the interaction term. Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "lr = LogisticRegression(verbose=0)\n",
    "model = make_pipeline(poly, lr).fit(X, Y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "A = model.predict_proba(meshData)[:, 1].reshape(xx.shape)\n",
    "Z = model.predict(meshData).reshape(xx.shape)\n",
    "ax.contourf(xx, yy, A, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X[:, 0], X[:, 1], color=Y, s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "for i in range(20000):\n",
    "    for solver in ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']:\n",
    "        X_transform = PolynomialFeatures(interaction_only=True, include_bias=False).fit_transform(X)\n",
    "        model = LogisticRegression(verbose=0, solver=solver, random_state=i, max_iter=10000)\n",
    "        model.fit(X_transform, Y)\n",
    "        print(model.score(X_transform, Y))\n",
    "        if model.score(X_transform, Y) > .75:\n",
    "            print(\"random state = \", i)\n",
    "            print(\"solver = \", solver)\n",
    "            break\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "xx, yy = np.meshgrid([x / 10 for x in range(-1, 11)], [x / 10 for x in range(-1, 11)])\n",
    "z = - model.intercept_ / model.coef_[0][2] - model.coef_[0][0] * xx / model.coef_[0][2] - model.coef_[0][1] * yy / model.coef_[0][2]\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X[: , 0], X[: , 1], X[: , 0]* X[: , 1], c=Y)\n",
    "ax.plot_surface(xx, yy, z, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Using the code below that generates 3 concentric circles, fit a logisitc regression model to it and plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate concentric circles data\n",
    "centers = [[0, 0], [0, 0], [0, 0]]\n",
    "t, _ = make_blobs(n_samples=1500, centers=centers, cluster_std=2, random_state=0)\n",
    "\n",
    "# CIRCLES\n",
    "def generate_circles_data(t):\n",
    "    def label(x):\n",
    "        if x[0]**2 + x[1]**2 >= 2 and x[0]**2 + x[1]**2 < 8:\n",
    "            return 1\n",
    "        if x[0]**2 + x[1]**2 >= 8:\n",
    "            return 2\n",
    "        return 0\n",
    "    # create some space between the classes\n",
    "    X = np.array(list(filter(lambda x: (x[0]**2 + x[1]**2 < 1.8 or x[0]**2 + x[1]**2 > 2.2) and (x[0]**2 + x[1]**2 < 7.8 or x[0]**2 + x[1]**2 > 8.2), t)))\n",
    "    Y = np.array([label(x) for x in X])\n",
    "    return X, Y\n",
    "\n",
    "# Generate circles data\n",
    "X, Y = generate_circles_data(t)\n",
    "\n",
    "# Fit logistic regression model to the data\n",
    "poly = PolynomialFeatures(1)  # Adjust the degree here if needed\n",
    "lr = LogisticRegression(verbose=2)\n",
    "model = make_pipeline(poly, lr).fit(X, Y)\n",
    "\n",
    "# Plot decision boundary\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "A = model.predict_proba(mesh_data)[:, 1].reshape(xx.shape)\n",
    "Z = model.predict(mesh_data).reshape(xx.shape)\n",
    "ax.contourf(xx, yy, A, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.coolwarm, s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Recall in Linear Regression we are trying to find the line $$y = X \\beta$$ that minimizes the sum of square distances between the predicted `y` and the `y` we observed in our dataset:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2$$\n",
    "\n",
    "We were able to find a global minimum to this loss function but we will try to apply gradient descent to find that same solution.\n",
    "\n",
    "a) Implement the `loss` function to complete the code and plot the loss as a function of beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install ipympl\n",
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta = np.array([ 1 , .5 ])\n",
    "xlin = -10.0 + 20.0 * np.random.random(100)\n",
    "X = np.column_stack([np.ones((len(xlin), 1)), xlin])\n",
    "y = beta[0]+(beta[1]*xlin)+np.random.randn(100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xlin, y,'ro',markersize=4)\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(min(y), max(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = np.arange(-5, 4, 0.1)\n",
    "b1 = np.arange(-5, 4, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "def loss(X, y, beta):\n",
    "    y_pred = X.dot(beta)\n",
    "    return np.sum((y_pred - y) ** 2) / (2 * len(y))\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    " \n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(14, 9))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-6, 4)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-5, 4)\n",
    "ax.set_zlim(0, 30000)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Since the loss is\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2 = \\beta^T X^T X \\beta - 2\\mathbf{\\beta}^TX^T\\mathbf{y}  + \\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = 2X^T X \\beta - 2X^T\\mathbf{y}$$\n",
    "\n",
    "b) Implement the gradient function below and complete the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(20, -20)\n",
    "    ax.set_xlim(-5, 4)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-5, 4)\n",
    "    ax.set_zlim(0, 30000)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    return 2 * X.T.dot(X).dot(beta) - 2 * X.T.dot(y)\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        grad = gradient(X, y, beta_hat)\n",
    "        beta_hat = beta_hat - learning_rate * grad\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.0002 # try .0005\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the code above to create an animation of the linear model learned at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snap_model(beta, xplot):\n",
    "    xplot = np.linspace(-10,10,50)\n",
    "    yestplot = beta[0] + beta[1] * xplot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xplot, yestplot,'b-',lw=2)\n",
    "    ax.plot(xlin, y,'ro',markersize=4)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(min(y), max(y))\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "    xplot = np.linspace(-10, 10, 50)  # x values for plotting the linear model\n",
    "\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap_model(beta_hat, xplot))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 100, images)\n",
    "\n",
    "images[0].save(\n",
    "    'model.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the `loss` is the negative log-likelihood\n",
    "\n",
    "$$ \\mathcal{l}(\\mathbf{\\beta}) = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\sigma(x_i \\beta)) + (1 - y_i)\\log(1 - \\sigma(x_i \\beta))$$\n",
    "\n",
    "the gradient of which is:\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{l}(\\mathbf{\\beta}) = \\frac{1}{N} \\sum_{i=1}^{N} x_i (y_i - \\sigma(x_i \\beta)) $$\n",
    "\n",
    "d) Plot the loss as a function of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "centers = [[0, 0]]\n",
    "t, _ = datasets.make_blobs(n_samples=100, centers=centers, cluster_std=2, random_state=0)\n",
    "\n",
    "# LINE\n",
    "def generate_line_data():\n",
    "    # create some space between the classes\n",
    "    X = t\n",
    "    Y = np.array([1 if x[0] - x[1] >= 0 else 0 for x in X])\n",
    "    return X, Y\n",
    "\n",
    "X, y = generate_line_data()\n",
    "\n",
    "cs = np.array([x for x in 'gb'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], color=cs[y].tolist(), s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b0 = np.arange(-20, 20, 0.1)\n",
    "b1 = np.arange(-20, 20, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(x)\n",
    "    return e / (1 + e)\n",
    "\n",
    "\n",
    "def loss(X, y, beta):\n",
    "    N = len(y)\n",
    "    y_pred = sigmoid(np.dot(X, beta))\n",
    "    epsilon = 1e-10  # Small epsilon value to avoid log(0) or log(1)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip to avoid exact 0 or 1\n",
    "    loss_val = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    return loss_val\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(14, 9))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-20, 20)\n",
    "ax.set_zlim(0, 10)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "e) Plot the loss at each iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(10, 10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-20, 20)\n",
    "    ax.set_zlim(0, 10)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    N = len(y)\n",
    "    y_pred = sigmoid(np.dot(X, beta))\n",
    "    gradient_val = 1 / N * np.dot(X.T, (y_pred - y))\n",
    "    return gradient_val\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.1\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd_logit.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "f) Create an animation of the logistic regression fit at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-10, 10)\n",
    "\n",
    "def snap(x, y, pts, losses, grad):\n",
    "    ax.clear()\n",
    "    ax.scatter(X[:, 0], X[:, 1], color=cs[y].tolist(), s=50, alpha=0.9)\n",
    "    ax.plot(np.array(pts)[:, 0], np.array(pts)[:, 1], 'o-', c='r', markersize=7)\n",
    "    ax.set_title(f'Logistic Regression Epoch {len(pts) - 1}\\nLoss: {losses[-1]}')\n",
    "    fig.canvas.draw()\n",
    "    return im.frombytes('RGB', fig.canvas.get_width_height(), fig.canvas.tostring_rgb())\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(X, y, betas, losses, gradient(X, y, beta_hat)))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses), images\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.1\n",
    "images = []\n",
    "betas, losses, images = gradient_descent(X, y, beta_start, learning_rate, 100, images)\n",
    "\n",
    "images[0].save(\n",
    "    'logistic_regression_animation.gif',\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "g) Modify the above code to evaluate the gradient on a random batch of the data. Overlay the true loss curve and the approximation of the loss in your animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "batch_size = 10  # Choose an appropriate batch size\n",
    "\n",
    "def gradient_descent_batch(X, y, beta_hat, learning_rate, epochs, batch_size, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        batch_indices = np.random.choice(len(X), size=batch_size, replace=False)\n",
    "        X_batch, y_batch = X[batch_indices], y[batch_indices]\n",
    "        \n",
    "        images.append(snap(X, y, betas, losses, gradient(X_batch, y_batch, beta_hat)))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X_batch, y_batch, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses), images\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.1\n",
    "images = []\n",
    "betas, losses, images = gradient_descent_batch(X, y, beta_start, learning_rate, 100, batch_size, images)\n",
    "\n",
    "images[0].save(\n",
    "    'logistic_regression_animation_batch.gif',\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Below is a sandox where you can get intuition about how to tune gradient descent parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(x, y, pts, losses, grad):\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(20, -20)\n",
    "    ax.plot_surface(x, y, loss(np.array([x, y])), color='r', alpha=.4)\n",
    "    ax.plot(np.array(pts)[:,0], np.array(pts)[:,1], losses, 'o-', c='b', markersize=10, zorder=10)\n",
    "    ax.plot(np.array(pts)[-1,0], np.array(pts)[-1,1], -1, 'o-', c='b', alpha=.5, markersize=7, zorder=10)\n",
    "    \n",
    "    # Plot Gradient Vector\n",
    "    X, Y, Z = [pts[-1][0]], [pts[-1][1]], [-1]\n",
    "    U, V, W = [-grad[0]], [-grad[1]], [0]\n",
    "    ax.quiver(X, Y, Z, U, V, W, color='g')\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def loss(x):\n",
    "    return np.sin(sum(x**2)) # change this\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * x * np.cos(sum(x**2)) # change this\n",
    "\n",
    "def gradient_descent(x, y, init, learning_rate, epochs):\n",
    "    images, losses, pts = [], [loss(init)], [init]\n",
    "    for _ in range(epochs):\n",
    "        grad = gradient(init)\n",
    "        images.append(snap(x, y, pts, losses, grad))\n",
    "        init = init - learning_rate * grad\n",
    "        losses.append(loss(init))\n",
    "        pts.append(init)\n",
    "    return images\n",
    "\n",
    "init = np.array([-.5, -.5]) # change this\n",
    "learning_rate = 1.394 # change this\n",
    "x, y = np.meshgrid(np.arange(-2, 2, 0.1), np.arange(-2, 2, 0.1)) # change this\n",
    "images = gradient_descent(x, y, init, learning_rate, 12)\n",
    "\n",
    "images[0].save(\n",
    "    'gradient_descent.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
